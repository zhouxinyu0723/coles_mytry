{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b4a8b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils import load_adj_neg, load_dataset_adj_lap\n",
    "from model import Encoder\n",
    "from model import Model\n",
    "import argparse\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "467b3274",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import pylab as pl\n",
    "from matplotlib import collections  as mc\n",
    "def visualize(x,y,labels,lap = None, width = 5):\n",
    "    ss = 1\n",
    "    #plt.scatter(emb_pca_x,emb_pca_y,s=ss,color='r')\n",
    "    label_num = labels.max().item()\n",
    "    colors = ['r','g','b','c','m','y','k','w']\n",
    "    if lap is not None:\n",
    "        a = lap_normalized.to_sparse()._indices()\n",
    "        lines = [[(x[a[0,i]],y[a[0,i]]),(x[a[1,i]],y[a[1,i]])] for i in range(a.shape[1])]\n",
    "        lc = mc.LineCollection(lines, colors='k', linewidths=width/10)\n",
    "        fig, ax = pl.subplots()\n",
    "        ax.add_collection(lc)\n",
    "        ax.autoscale()\n",
    "        ax.margins(0.1)\n",
    "    for i in range(label_num):\n",
    "        plt.scatter(x[np.nonzero(labels == i)],y[np.nonzero(labels == i)],s=ss,color=colors[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85c127cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/time_eternity/desktop_download_doc_pic_vid_music/Desktop/fp_jupyter_conda/coles_orthonormal_and_poisson/coles_mytry/utils.py:193: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Splits are randomized and results might slightly deviate from reported numbers in the paper.\n",
    "import torch\n",
    "from utils import load_adj_neg, load_dataset_adj_lap\n",
    "from ssgc import Net, MLP\n",
    "import argparse\n",
    "import numpy as np\n",
    "from classification import classify\n",
    "parser = argparse.ArgumentParser()\n",
    "dataset = 'cora'\n",
    "if dataset=='cora':\n",
    "    nhid = 512\n",
    "    output = 512\n",
    "    num_nodes = 2708\n",
    "    num_features = 1433\n",
    "    num_class = 7\n",
    "    lr = 0.001\n",
    "    weight_decay = 5e-4\n",
    "    epoch = 40\n",
    "    sample = 2\n",
    "if dataset=='citeseer':\n",
    "    nhid = 512\n",
    "    output = 512\n",
    "    num_nodes = 3327\n",
    "    num_features = 3703\n",
    "    lr = 0.0001\n",
    "    weight_decay = 1e-4\n",
    "    epoch = 160\n",
    "    sample = 5\n",
    "if dataset=='pubmed':\n",
    "    nhid = 256\n",
    "    output = 256\n",
    "    num_nodes = 19717\n",
    "    num_features = 500\n",
    "    lr = 0.02\n",
    "    weight_decay = 1e-5\n",
    "    epoch = 40\n",
    "    sample = 3\n",
    "parser.add_argument('--dataset', type=str, default=dataset,\n",
    "                    help='dataset')\n",
    "parser.add_argument('--seed', type=int, default=123,\n",
    "                    help='seed')\n",
    "parser.add_argument('--nhid', type=int, default=nhid,\n",
    "                    help='hidden size')\n",
    "parser.add_argument('--output', type=int, default=output,\n",
    "                    help='output size')\n",
    "parser.add_argument('--lr', type=float, default=lr,\n",
    "                    help='learning rate')\n",
    "parser.add_argument('--weight_decay', type=float, default=weight_decay,\n",
    "                    help='weight decay')\n",
    "parser.add_argument('--epochs', type=int, default=epoch,\n",
    "                    help='maximum number of epochs')\n",
    "parser.add_argument('--sample', type=int, default=sample,\n",
    "                    help='    ')\n",
    "parser.add_argument('--num_nodes', type=int, default=num_nodes,\n",
    "                    help='    ')\n",
    "parser.add_argument('--num_features', type=int, default=num_features,\n",
    "                    help='    ')\n",
    "\n",
    "label_file = open(\"data/{}{}\".format(dataset,\"_labels.txt\"), 'r')\n",
    "label_text = label_file.readlines()\n",
    "labels = []\n",
    "for line in label_text:\n",
    "    if line.strip('\\n'):\n",
    "        line = line.strip('\\n').split(' ')\n",
    "        labels.append(int(line[1]))\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "args.device = 'cpu'\n",
    "torch.manual_seed(args.seed)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "feature, adj_normalized, lap_normalized= load_dataset_adj_lap(args.dataset)\n",
    "feature = feature.to(device)\n",
    "adj_normalized = adj_normalized.to(device)\n",
    "lap_normalized = lap_normalized.to(device)\n",
    "D = torch.unsqueeze(torch.sum(lap_normalized>0,dim=0),1)\n",
    "\n",
    "K = 8\n",
    "\n",
    "feature_poisson_origin = F.normalize(feature)\n",
    "feature_poisson_origin = feature_poisson_origin - torch.mean(feature_poisson_origin,dim=0)\n",
    "feature_smoothed_poisson = torch.zeros(feature.shape)\n",
    "adj_poisson = (adj_normalized>0)/D\n",
    "adj_extrinsic_poisson = adj_poisson * (1 - torch.eye(adj_poisson.shape[0]))\n",
    "\n",
    "for i in range(20):\n",
    "    feature_smoothed_poisson = feature_poisson_origin/D + torch.mm(adj_extrinsic_poisson, feature_smoothed_poisson)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d120e27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.501647446458\n",
      "1.1428995798576929\n",
      "76.51099317664897\n",
      "2.320382303220732\n"
     ]
    }
   ],
   "source": [
    "classify(F.normalize(feature_smoothed_poisson)*(torch.sqrt(D)), args.dataset, per_class='20')\n",
    "classify(F.normalize(feature_smoothed_poisson)*(torch.sqrt(D)), args.dataset, per_class='5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a7d152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "466f638a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7149, dtype=torch.float64)\n",
      "tensor(0.7339, dtype=torch.float64)\n",
      "tensor(0.7180, dtype=torch.float64)\n",
      "tensor(0.7726, dtype=torch.float64)\n",
      "tensor(0.7722, dtype=torch.float64)\n",
      "tensor(0.6778, dtype=torch.float64)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [38]\u001b[0m, in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m#loss = -torch.trace(torch.mm(out[train_ids],torch.transpose(torch.tensor(labels_onehot[train_ids]).float(),0,1))/out.shape[1])#*torch.sqrt(D[train_ids]))\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 38\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     40\u001b[0m acc \u001b[38;5;241m=\u001b[39m accuracy( F\u001b[38;5;241m.\u001b[39mlog_softmax(model(feature_smoothed_poisson_trick[test_ids]),dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), torch\u001b[38;5;241m.\u001b[39mtensor(labels[test_ids]))\n",
      "File \u001b[0;32m~/anaconda3/envs/condenv/lib/python3.9/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/condenv/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/condenv/lib/python3.9/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[38;5;66;03m# record the step after step update\u001b[39;00m\n\u001b[1;32m    139\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 141\u001b[0m     \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m           \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m           \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m           \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m           \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m           \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/condenv/lib/python3.9/site-packages/torch/optim/_functional.py:97\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m     94\u001b[0m     grad \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39madd(param, alpha\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m \u001b[43mexp_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[1;32m     98\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# Maintains the maximum of all 2nd moment running avg. till now\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "per_class = '5'\n",
    "label_file = open(\"data/{}{}\".format(dataset,\"_labels.txt\"), 'r')\n",
    "label_text = label_file.readlines()\n",
    "labels = []\n",
    "for line in label_text:\n",
    "    if line.strip('\\n'):\n",
    "        line = line.strip('\\n').split(' ')\n",
    "        labels.append(int(line[1]))\n",
    "label_file.close()\n",
    "labels = np.array(labels)\n",
    "train_file = open(\"data/{}/{}/train_text.txt\".format(dataset, per_class), 'r')\n",
    "train_text = train_file.readlines()\n",
    "train_file.close()\n",
    "test_file = open( \"data/{}/{}/test_text.txt\".format(dataset, per_class), 'r')\n",
    "test_text = test_file.readlines()\n",
    "test_file.close()\n",
    "\n",
    "feature_smoothed_poisson_trick = F.normalize(feature_smoothed_poisson)*(torch.sqrt(D))\n",
    "accs = []\n",
    "for k in range(50):\n",
    "    train_ids = eval(train_text[k])\n",
    "    test_ids = eval(test_text[k])\n",
    "\n",
    "    labels_onehot = encode_onehot(labels)\n",
    "    model = MLP(args,num_class).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "    for epoch in range(50):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(feature_smoothed_poisson_trick[train_ids])\n",
    "        \n",
    "        out = F.log_softmax(out,dim=1)\n",
    "\n",
    "        loss = -torch.trace(torch.mm(out,torch.transpose(torch.tensor(labels_onehot[train_ids]).float(),0,1))/out.shape[1]*D[train_ids])\n",
    "        #loss = -torch.trace(torch.mm(out[train_ids],torch.transpose(torch.tensor(labels_onehot[train_ids]).float(),0,1))/out.shape[1])#*torch.sqrt(D[train_ids]))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.eval()\n",
    "    acc = accuracy( F.log_softmax(model(feature_smoothed_poisson_trick[test_ids]),dim=1), torch.tensor(labels[test_ids]))\n",
    "    accs.append(acc)\n",
    "    print(acc)\n",
    "    model.eval()\n",
    "    emb_origin = model(feature_smoothed_poisson_trick).cpu().detach().numpy()\n",
    "acc_mean = sum(acc)/len(acc)\n",
    "print(acc_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55208e5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.333333333333333"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a79139a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4, 4,  ..., 6, 3, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.log_softmax(model(feature_smoothed_poisson),dim=1)\n",
    "preds = F.log_softmax(model(feature_smoothed_poisson),dim=1).max(1)[1]\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e215d8e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 4, ..., 3, 3, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2644b6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.transpose(torch.tensor(labels_onehot[train_ids]).float(),0,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a07301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import FastICA\n",
    "transformer = decomposition.PCA(n_components=7,\n",
    "        random_state=0)\n",
    "feature_pca = transformer.fit_transform(emb_origin)\n",
    "#feature_pca = transformer.fit_transform(feature_smoothed_poisson*(torch.sqrt(D)))\n",
    "visualize(feature_pca[:,0],feature_pca[:,1],labels,lap_normalized,0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaf99b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(feature_smoothed_poisson, args.dataset, per_class='20')\n",
    "classify(feature_smoothed_poisson, args.dataset, per_class='5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b382e20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#feature_pca = transformer.fit_transform(F.normalize(feature_smoothed_sgc))\n",
    "feature_pca = transformer.fit_transform(feature_smoothed_poisson/D)\n",
    "visualize(feature_pca[:,3],feature_pca[:,4],labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a19211",
   "metadata": {},
   "outputs": [],
   "source": [
    "cora without orth\n",
    "82.26276771004942\n",
    "1.254241627563159\n",
    "77.16148597422288\n",
    "2.6389531638400907\n",
    "with orth\n",
    "81.75947281713344\n",
    "1.246101318967512\n",
    "72.3131159969674\n",
    "4.046646313156723\n",
    "with raw feature_smoothed_ssgc\n",
    "81.59390444810545\n",
    "1.2038835463852553\n",
    "71.03032600454888\n",
    "4.572080755555725\n",
    "\n",
    "citeseer without orth\n",
    "70.42112082928408\n",
    "1.1220317077364168\n",
    "66.50749923477197\n",
    "2.431697731089557\n",
    "with orth\n",
    "70.94978943958535\n",
    "1.0752492722237095\n",
    "66.09978573614937\n",
    "2.540035128432411\n",
    "with raw feature_smoothed_ssgc\n",
    "69.89050858438614\n",
    "1.2455656779678324\n",
    "60.153657790021434\n",
    "4.213946942708061\n",
    "\n",
    "\n",
    "pubmed without orth\n",
    "40 epoch\n",
    "74.67142930040312   claimed 77.4±1.9 \n",
    "2.381893996454681\n",
    "64.65291816935033   claimed 66.0±5.2b\n",
    "5.362018399570344\n",
    "\n",
    "with ort\n",
    "40 epoch\n",
    "74.04715007399092\n",
    "2.5807319620069507\n",
    "66.49210138670189\n",
    "4.908270660412052\n",
    "\n",
    "raw feature feature_smoothed_ssgc\n",
    "74.04715007399092\n",
    "2.5807319620069507\n",
    "66.49210138670189\n",
    "4.908270660412052\n",
    "\n",
    "pubmed 80 epoch wihtout orth\n",
    "73.68137980303108    \n",
    "2.4570598828041033\n",
    "63.52598161223142    \n",
    "5.489841045145188\n",
    "\n",
    "pubmed 80 epoch with orth\n",
    "72.51885492677451\n",
    "2.5179344333050375\n",
    "66.14364809264997\n",
    "4.886909664894386"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
